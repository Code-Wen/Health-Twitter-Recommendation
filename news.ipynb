{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "Twitter has already become one of the most popular news and social networking online service. Almost all main-stream news agencies have their Twitter accounts to broadcast news in different topics. \n",
    "\n",
    "The number of tweets per hour is massive, thus it would be ideal to develop a **recommendation system** for Twitter users which suggests the most relevant tweets based on the current content. We try to use **Natural Language Processing (NLP)** techniques to achieve this goal here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First glance of the data\n",
    "\n",
    "Our data comes from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/index.php, a fantastic source of data sets for the machine learning community. \n",
    "\n",
    "The dataset at hand contains health news from more than 15 major health news agencies such as BBC, CNN, and NYT in 2015. The folder `Health-Tweets` contains 16 files in the `.txt` format, and each file stores the Health related tweets for one news agency. Each line contains the information of a single tweet `id|date and time|tweet content`. The separator is '|'.\n",
    "\n",
    "As the first step,  we would like to take a look at the names of the files. Let's list all the `.txt` files and sort them alphabetically using `glob`.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Health-Tweets\\\\bbchealth.txt', 'Health-Tweets\\\\cbchealth.txt', 'Health-Tweets\\\\cnnhealth.txt', 'Health-Tweets\\\\everydayhealth.txt', 'Health-Tweets\\\\foxnewshealth.txt', 'Health-Tweets\\\\gdnhealthcare.txt', 'Health-Tweets\\\\goodhealth.txt', 'Health-Tweets\\\\KaiserHealthNews.txt', 'Health-Tweets\\\\latimeshealth.txt', 'Health-Tweets\\\\msnhealthnews.txt', 'Health-Tweets\\\\NBChealth.txt', 'Health-Tweets\\\\nprhealth.txt', 'Health-Tweets\\\\nytimeshealth.txt', 'Health-Tweets\\\\reuters_health.txt', 'Health-Tweets\\\\usnewshealth.txt', 'Health-Tweets\\\\wsjhealth.txt']\n"
     ]
    }
   ],
   "source": [
    "# Import library\n",
    "import glob\n",
    "\n",
    "# The books files are contained in this folder\n",
    "folder = \"Health-Tweets/\"\n",
    "\n",
    "# List all the .txt files and sort them alphabetically\n",
    "files = glob.glob(folder+'*.txt')\n",
    "# print files\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing the data into Python\n",
    "\n",
    "Next, we need to load the content of these books into Python and do some basic pre-processing to facilitate the downstream analyses. We call such a collection of texts a corpus. Our recommendation system is unbiased towards different news agencies, hence we store all the tweets in a single list `txts`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63327\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import re, os\n",
    "\n",
    "# Initialize the object that will contain the texts and titles\n",
    "txts = []\n",
    "titles = []\n",
    "\n",
    "for n in files:\n",
    "    # Open each file\n",
    "    f = open(n)\n",
    "    txt = f.read()\n",
    "    # Store the texts and titles of the books in two separate lists\n",
    "    txts.extend(txt.splitlines())\n",
    "    \n",
    "#Â Print the number of all tweets\n",
    "print(len(txts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenize the corpus\n",
    "\n",
    "As a next step, we need to transform the corpus into a format that is easier to deal with for the downstream analyses. We will tokenize our corpus, i.e., transform each text into a list of the individual words (called tokens) it is made of. \n",
    "\n",
    "To this end, we first build a set of stop words which are non-informative called `stoplist`. Then, we remove the empty lines from the `txts` list and call the resulting list `txts_orig`, which stores all the original information. Next, we remove all the non-alpha-numeric characters except the in-line separator '|' and then change everything to lower case.  For simplicity, we will drop the idenfication number and the date/time, and focus on the contents of the tweets. After splitting the texts in each tweet, we remove the stop words and obtain our list of tokens called `texts`. \n",
    "\n",
    "To see what the elements in the list `texts` look like, we print out the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drugs', 'need', 'careful', 'monitoring', 'expiry', 'dates', 'pharmacists', 'say', 'cbc', 'ca', 'news', 'health', 'drugs', 'need', 'careful', 'monitoring', 'expiry', 'dates', 'pharmacists', 'say', '1', '3026749', 'cmp', 'rss']\n"
     ]
    }
   ],
   "source": [
    "# Define a list of stop words\n",
    "stoplist = set('http www com html for a of the and to in to be which some is at that we i who whom show via may my our might as well'.split())\n",
    "\n",
    "# clean out tweets without two vertical lines\n",
    "txts_orig = [i for i in txts if i.count('|')==2]\n",
    "\n",
    "# Remove all non-alpha-numeric characters\n",
    "txt = [ re.sub('[^|a-zA-Z0-9\\s]', ' ', i) for i in txts_orig ]\n",
    "    \n",
    "# Convert the text to lower case \n",
    "txts_lower_case = [i.lower() for i in txt]\n",
    "    \n",
    "# Transform the text into tokens \n",
    "txts_split = [ i.split('|')[2].split() for i in txts_lower_case]\n",
    "\n",
    "# Remove tokens which are part of the list of stop words\n",
    "texts = [ [w for w in i if w not in stoplist] for i in txts_split]\n",
    "\n",
    "# Print the first 20 tokens for the first file\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stemming of the tokenized corpus\n",
    "\n",
    "One usually use different words to refer to a similar concept. This will dilute the weight given to this concept in the tweet and potentially bias the results of the analysis.\n",
    "\n",
    "To solve this issue, it is a common practice to use a stemming process, which will group together the inflected forms of a word so they can be analysed as a single item: the stem. The package `nltk` provides an easy method of generating the stems. The stems are stored in the list `texts_stem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Porter stemming function from the nltk package\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Create an instance of a PorterStemmer object\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# For each token of each text, we generated its stem \n",
    "texts_stem = [[porter.stem(token) for token in text] for text in texts]\n",
    "\n",
    "# Print the first stemmed tweet\n",
    "print(texts_stem[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building a bag-of-word model\n",
    "\n",
    "First, we need to create a universe of all words contained in our corpus of tweets, which we call a dictionary. Then, using the stemmed tokens and the dictionary, we will create bag-of-words models (BoW) for each of our tweets. The BoW models will represent our tweets as a list of all uniques tokens they contain associated with their respective number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 2), (4, 1), (5, 1), (6, 2), (7, 2), (8, 2), (9, 1), (10, 2), (11, 2), (12, 1), (13, 2), (14, 1), (15, 2)]\n"
     ]
    }
   ],
   "source": [
    "# Load the functions allowing to create and use dictionaries\n",
    "from gensim import corpora\n",
    "\n",
    "# Create a dictionary from the stemmed tokens\n",
    "dictionary = corpora.Dictionary(texts_stem)\n",
    "\n",
    "# Create a bag-of-words model for each tweet, using the previously generated dictionary\n",
    "bows = [dictionary.doc2bow(i) for i in texts_stem]\n",
    "\n",
    "# Print the first tweet's BoW model\n",
    "print(bows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The most common words of a given tweet\n",
    "\n",
    "The results returned by the bag-of-words model is certainly easy to use for a computer but hard to interpret for a human. It is not straightforward to understand which stemmed tokens are present in a given tweet, and how many occurrences we can find.\n",
    "\n",
    "In order to better understand how the model has been generated and visualize its content, we will transform it into a DataFrame, and sort the occurrences in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index  occurrences       token\n",
      "3       3            2        care\n",
      "6       6            2        date\n",
      "7       7            2        drug\n",
      "8       8            2      expiri\n",
      "10     10            2     monitor\n",
      "11     11            2        need\n",
      "13     13            2  pharmacist\n",
      "15     15            2         say\n",
      "0       0            1           1\n",
      "1       1            1     3026749\n",
      "2       2            1          ca\n",
      "4       4            1         cbc\n",
      "5       5            1         cmp\n",
      "9       9            1      health\n",
      "12     12            1        news\n",
      "14     14            1         rss\n"
     ]
    }
   ],
   "source": [
    "# Import pandas to create and manipulate DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the BoW model for first tweet\n",
    "df_bow_0 = pd.DataFrame(bows[0])\n",
    "\n",
    "# Add the column names to the DataFrame\n",
    "df_bow_0.columns = ['index','occurrences']\n",
    "\n",
    "# Add a column containing the token corresponding to the dictionary index\n",
    "df_bow_0['token'] = [dictionary[i] for i in df_bow_0['index']]\n",
    "\n",
    "# Sort the DataFrame by descending number of occurrences and print \n",
    "df_bow_0.sort_values(by = 'occurrences', ascending = False, inplace=True)\n",
    "print(df_bow_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build a tf-idf model\n",
    "\n",
    "Some of the most recurring words are very common and unlikely to carry any information peculiar to the given tweet. We need to use an additional step in order to determine which tokens are the most specific to a book.\n",
    "\n",
    "To do so, we will use a tf-idf model (term frequencyâinverse document frequency). This model defines the importance of each word depending on how frequent it is in this text and how infrequent it is in all the other documents. As a result, a high tf-idf score for a word will indicate that this word is specific to this text. This can be done with the help of the package `gensim`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.08581568812538018), (1, 0.30253680106149267), (2, 0.094606136667875998), (3, 0.1928206151948339), (4, 0.094882487238185015), (5, 0.096016664664067899), (6, 0.35873339826173772), (7, 0.1826391735878661), (8, 0.54493053053055707), (9, 0.061690540424185959), (10, 0.33945122875109651), (11, 0.21504701074076696), (12, 0.090668982336277057), (13, 0.41043755517197905), (14, 0.09626385303997112), (15, 0.1628941252282041)]\n"
     ]
    }
   ],
   "source": [
    "# Load the gensim functions that will allow us to generate tf-idf models\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Generate the tf-idf model\n",
    "model = TfidfModel(corpus=bows)\n",
    "\n",
    "# Print the model for the first tweet\n",
    "print(model[bows[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The results of the tf-idf model\n",
    "Once again, the format of those results is hard to interpret for a human. Therefore, we will transform it into a more readable version and display the words from the first tweet with their tf-idf scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id     score       token\n",
      "8    8  0.544931      expiri\n",
      "13  13  0.410438  pharmacist\n",
      "6    6  0.358733        date\n",
      "10  10  0.339451     monitor\n",
      "1    1  0.302537     3026749\n",
      "11  11  0.215047        need\n",
      "3    3  0.192821        care\n",
      "7    7  0.182639        drug\n",
      "15  15  0.162894         say\n",
      "14  14  0.096264         rss\n",
      "5    5  0.096017         cmp\n",
      "4    4  0.094882         cbc\n",
      "2    2  0.094606          ca\n",
      "12  12  0.090669        news\n",
      "0    0  0.085816           1\n",
      "9    9  0.061691      health\n"
     ]
    }
   ],
   "source": [
    "# Convert the tf-idf model for the first tweet into a DataFrame\n",
    "df_tfidf = pd.DataFrame(model[bows[0]])\n",
    "\n",
    "# Name the columns of the DataFrame id and score\n",
    "df_tfidf.columns = ['id','score']\n",
    "\n",
    "# Add the tokens corresponding to the numerical indices for better readability\n",
    "df_tfidf['token'] = [dictionary[i] for i in df_tfidf['id']]\n",
    "\n",
    "# Sort the DataFrame by descending tf-idf score and print the first 10 rows.\n",
    "df_tfidf.sort_values(by = 'score', ascending=False, inplace=True)\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compute distance between texts\n",
    "\n",
    "The results of the tf-idf algorithm now return stemmed tokens which are specific to each tweet.  Now that we have a model associating tokens to how specific they are to each tweet, we can measure how each tweet is related to the given tweet.\n",
    "\n",
    "To this purpose, we will use a measure of similarity called **cosine similarity** and we will visualize the results as a distance matrix, i.e., a matrix showing all distances between each tweet and the given tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the library allowing similarity computations\n",
    "from gensim import similarities\n",
    "\n",
    "# Compute the similarity matrix (pairwise distance between all texts)\n",
    "corpus_tfidf = model[bows]\n",
    "#sims = similarities.MatrixSimilarity(corpus=corpus_tfidf)\n",
    "sims = similarities.Similarity('/Users/mshen/Desktop/',corpus_tfidf, num_features=len(dictionary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Recommending the most related tweets\n",
    "\n",
    "Now we are set! Once we have the similarity scores between the current tweet and each tweet in the dataset, we can sort the scores and get the top `n` tweets. Those should be the most related tweets and will likely be of interest to the reader! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current tweet: \n",
      "585906983091376128|Wed Apr 08 20:48:05 +0000 2015|Check expiry dates, Health Canada advises after Alesse 21 birth control pill recall http://www.cbc.ca/news/health/alesse-21-birth-control-pills-recalled-in-western-canada-1.3025078?cmp=rss\n",
      "\n",
      "The top 5 similar tweets:\n",
      "585579956371066880|Tue Apr 07 23:08:35 +0000 2015|Expired Alesse birth control exposes 'deficiency' http://www.cbc.ca/news/health/expired-alesse-birth-control-exposes-deficiency-1.3023939?cmp=rss\n",
      "585416006467624960|Tue Apr 07 12:17:07 +0000 2015|Shoppers Drug Mart mistakenly sells expired birth control pills in Western Canada http://www.cbc.ca/news/canada/edmonton/shoppers-drug-mart-warns-of-expired-alesse-birth-control-pills-in-western-canada-1.3022846?cmp=rss\n",
      "521665128786178048|Mon Oct 13 14:13:53 +0000 2014|Birth control pill threatens fish populations http://www.cbc.ca/news/technology/birth-control-pill-threatens-fish-populations-1.2796897?cmp=rss\n",
      "377881233252155393|Wed Sep 11 19:48:01 +0000 2013|Birth control pill recalls show 'weak link in chain' http://bit.ly/1b6s4ch\n",
      "164768791820124160|Wed Feb 01 17:55:15 +0000 2012|Pfizer Recalls 1 Million Packets of Birth Control Pills:  http://on-msn.com/wSMB0u\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 5 # the number of top similar tweets to output\n",
    "i = 10 # the index of the current tweet to look for similar ones\n",
    "\n",
    "query_doc = model[bows[i]] # tf-idf for the current tweet\n",
    "s = sims[query_doc] # similarity array\n",
    "x = sorted(s, reverse=True) # sorted similarity array\n",
    "\n",
    "thres = x[n] # the threshold similairty for top n similar tweets\n",
    "ind = s >= thres # indices of the top similar tweet\n",
    "filtered = np.array(txts_orig)[ind]\n",
    "\n",
    "# print the current tweet\n",
    "print ('The current tweet: \\n' + filtered[0])\n",
    "\n",
    "# print the top n similar tweets\n",
    "print('\\nThe top ' + str(n) + ' similar tweets:')\n",
    "for i in filtered[1:]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Discussions:\n",
    "\n",
    "- Because there are so few words in each tweet, the results may be very unstable.\n",
    "\n",
    "- We did not take the date/time of the tweets into consideration, for simplicity purposes. However one can easily extract the date/time information and build a recommendation system which only include the recent tweets.\n",
    "\n",
    "- We can also analyse the emphasis of each news agency on Health and build a recommendation system for the Twitter users on news agencies. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
